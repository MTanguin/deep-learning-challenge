{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUPxIInNoK8C",
    "outputId": "d1e6852e-ebf9-4b75-c95c-98310499c546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 400       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 441\n",
      "Trainable params: 441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "684/684 [==============================] - 11s 13ms/step - loss: 0.6453 - accuracy: 0.6536 - val_loss: 0.5964 - val_accuracy: 0.7124\n",
      "Epoch 2/50\n",
      "684/684 [==============================] - 8s 12ms/step - loss: 0.5846 - accuracy: 0.7131 - val_loss: 0.5595 - val_accuracy: 0.7346\n",
      "Epoch 3/50\n",
      "684/684 [==============================] - 3s 4ms/step - loss: 0.5668 - accuracy: 0.7187 - val_loss: 0.5512 - val_accuracy: 0.7349\n",
      "Epoch 4/50\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5613 - accuracy: 0.7197 - val_loss: 0.5495 - val_accuracy: 0.7326\n",
      "Epoch 5/50\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5586 - accuracy: 0.7196 - val_loss: 0.5494 - val_accuracy: 0.7323\n",
      "Epoch 6/50\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5568 - accuracy: 0.7218 - val_loss: 0.5479 - val_accuracy: 0.7321\n",
      "Epoch 7/50\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5554 - accuracy: 0.7228 - val_loss: 0.5471 - val_accuracy: 0.7336\n",
      "Epoch 8/50\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5542 - accuracy: 0.7231 - val_loss: 0.5458 - val_accuracy: 0.7346\n",
      "Epoch 9/50\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5536 - accuracy: 0.7229 - val_loss: 0.5459 - val_accuracy: 0.7321\n",
      "Epoch 10/50\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5528 - accuracy: 0.7233 - val_loss: 0.5455 - val_accuracy: 0.7321\n",
      "Epoch 11/50\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5522 - accuracy: 0.7228 - val_loss: 0.5446 - val_accuracy: 0.7341\n",
      "Epoch 12/50\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5522 - accuracy: 0.7238 - val_loss: 0.5455 - val_accuracy: 0.7336\n",
      "Epoch 13/50\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5521 - accuracy: 0.7243 - val_loss: 0.5456 - val_accuracy: 0.7336\n",
      "Epoch 14/50\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5520 - accuracy: 0.7243 - val_loss: 0.5463 - val_accuracy: 0.7339\n",
      "Epoch 15/50\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5515 - accuracy: 0.7243 - val_loss: 0.5457 - val_accuracy: 0.7326\n",
      "Epoch 16/50\n",
      "684/684 [==============================] - 2s 3ms/step - loss: 0.5510 - accuracy: 0.7251 - val_loss: 0.5430 - val_accuracy: 0.7341\n",
      "Epoch 17/50\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5508 - accuracy: 0.7242 - val_loss: 0.5446 - val_accuracy: 0.7326\n",
      "Epoch 18/50\n",
      "684/684 [==============================] - 2s 2ms/step - loss: 0.5508 - accuracy: 0.7248 - val_loss: 0.5458 - val_accuracy: 0.7328\n",
      "Epoch 19/50\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5502 - accuracy: 0.7244 - val_loss: 0.5443 - val_accuracy: 0.7354\n",
      "Epoch 20/50\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5501 - accuracy: 0.7277 - val_loss: 0.5436 - val_accuracy: 0.7336\n",
      "Epoch 21/50\n",
      "684/684 [==============================] - 1s 2ms/step - loss: 0.5498 - accuracy: 0.7278 - val_loss: 0.5437 - val_accuracy: 0.7359\n"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import and read the charity_data.csv\n",
    "url = \"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\"\n",
    "application_df = pd.read_csv(url)\n",
    "\n",
    "# Create a new DataFrame with only the columns needed for modeling\n",
    "model_df = application_df.drop(columns=[\"EIN\", \"NAME\"])\n",
    "\n",
    "# Determine the number of unique values in each column\n",
    "unique_value_counts = model_df.nunique()\n",
    "\n",
    "# Binning for APPLICATION_TYPE column\n",
    "counts = model_df[\"APPLICATION_TYPE\"].value_counts()\n",
    "application_types_to_replace = list(counts[counts < 500].index)\n",
    "model_df[\"APPLICATION_TYPE\"] = model_df[\"APPLICATION_TYPE\"].replace(application_types_to_replace, \"Other\")\n",
    "\n",
    "# Binning for CLASSIFICATION column\n",
    "counts_binning = model_df[\"CLASSIFICATION\"].value_counts()\n",
    "counts_classification = counts_binning[counts_binning > 1]\n",
    "classifications_to_replace = list(counts_binning[counts_binning < 100].index)\n",
    "model_df[\"CLASSIFICATION\"] = model_df[\"CLASSIFICATION\"].replace(classifications_to_replace, \"Other\")\n",
    "\n",
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "model_df = pd.get_dummies(model_df, dtype=np.float32, sparse=True)\n",
    "\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "y = model_df[\"IS_SUCCESSFUL\"].values\n",
    "X = model_df.drop(\"IS_SUCCESSFUL\", axis=1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 4\n",
    "nn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, validation_split=0.15, epochs=50, batch_size=32, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "cu8ZBZGjo3dM",
    "outputId": "bbd42f07-fa75-49f5-c4a3-6a1af98ae65c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 0s - loss: 0.5540 - accuracy: 0.7240 - 360ms/epoch - 1ms/step\n",
      "Loss: 0.5540175437927246, Accuracy: 0.7239649891853333\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_b48e8a11-4236-4ccc-bb59-830b90fd6ffa\", \"AlphabetSoupCharity2.h5\", 36288)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "from google.colab import files\n",
    "\n",
    "nn.save('AlphabetSoupCharity2.h5')\n",
    "files.download('AlphabetSoupCharity2.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
